
<p align="center">
  <img src="https://github.com/AdhyaSuman/DTECT/blob/main/assets/Logo_light.png" width="400"/>
</p>

-----

## ğŸ‘‹ Introduction

> **DTECT (Dynamic Topic Explorer & Context Tracker)** is an end-to-end, open-source system designed to streamline the entire process of dynamic topic modeling.

-----

## ğŸš€ Live Demo & Video

  * **Interactive Demo:** Try DTECT live on Hugging Face Spaces\!

      * [https://huggingface.co/spaces/AdhyaSuman/DTECT](https://huggingface.co/spaces/AdhyaSuman/DTECT)

* **Demo Video:** Watch a walkthrough of DTECT's features.



Â  Â  [![Watch the Demo Video](https://img.youtube.com/vi/B8nNfxFoJAU/0.jpg)](https://www.youtube.com/watch?v=B8nNfxFoJAU)

-----

## ğŸ’» Code Examples

### DTECT Preprocessing Pipeline

Here is an example of how to use the DTECT preprocessing pipeline for a custom dataset:

```python
from backend.datasets.preprocess import Preprocessor
from nltk.corpus import stopwords
import os

dataset_dir = '../data/Sample_data/'
stop_words = stopwords.words('english')

preprocessor = Preprocessor(
    docs_jsonl_path=dataset_dir + 'docs.jsonl',
    output_folder=os.path.join(dataset_dir, 'processed'),
    use_partition=False,
    min_count_bigram=5,
    threshold_bigram=20,
    remove_punctuation=True,
    lemmatize=True,
    stopword_list=stop_words,
    min_chars=3,
    min_words_docs=3,
)
preprocessor.preprocess()
```

### Training and Evaluation Pipeline (with CFDTM)

This snippet shows an example of the training and evaluation pipeline using the CFDTM model:

```python
from backend.datasets import dynamic_dataset
from backend.models.CFDTM.CFDTM import CFDTM
from backend.models.dynamic_trainer import DynamicTrainer
from backend.evaluation.eval import TopicQualityAssessor

# Load dataset
data = dynamic_dataset.DynamicDataset('../data/Sample_data/processed')

# Initialize model
model = CFDTM(
    vocab_size=data.vocab_size,
    num_times=data.num_times,
    num_topics=20,
    pretrained_WE=data.pretrained_WE,
    train_time_wordfreq=data.train_time_wordfreq
).to("cuda")

# Train model
trainer = DynamicTrainer(model, data)
top_words, _ = trainer.train()
top_words_list = [[topic.split() for topic in timestamp] for timestamp in top_words]
train_corpus = [doc.split() for doc in data.train_texts]

# Evaluation
assessor = TopicQualityAssessor(
    topics=top_words_list,
    train_texts=train_corpus,
    topn=10,
    coherence_type='c_npmi'
)
summary = assessor.get_dtq_summary()
```

-----

## ğŸ“ Repository Structure

```
â”œâ”€â”€ app
â”‚   â””â”€â”€ ui.py
â”œâ”€â”€ assets
â”‚   â”œâ”€â”€ Logo_dark.png
â”‚   â””â”€â”€ Logo_light.png
â”œâ”€â”€ backend
â”‚   â”œâ”€â”€ datasets
â”‚   â”‚   â”œâ”€â”€ data
â”‚   â”‚   â”‚   â”œâ”€â”€ download.py
â”‚   â”‚   â”‚   â””â”€â”€ file_utils.py
â”‚   â”‚   â”œâ”€â”€ dynamic_dataset.py
â”‚   â”‚   â”œâ”€â”€ preprocess.py
â”‚   â”‚   â””â”€â”€ utils
â”‚   â”‚       â”œâ”€â”€ logger.py
â”‚   â”‚       â””â”€â”€ _utils.py
â”‚   â”œâ”€â”€ evaluation
â”‚   â”‚   â”œâ”€â”€ CoherenceModel_ttc.py
â”‚   â”‚   â”œâ”€â”€ eval.py
â”‚   â”‚   â”œâ”€â”€ topic_coherence.py
â”‚   â”‚   â””â”€â”€ topic_diversity.py
â”‚   â”œâ”€â”€ inference
â”‚   â”‚   â”œâ”€â”€ doc_retriever.py
â”‚   â”‚   â”œâ”€â”€ indexing_utils.py
â”‚   â”‚   â”œâ”€â”€ peak_detector.py
â”‚   â”‚   â”œâ”€â”€ process_beta.py
â”‚   â”‚   â””â”€â”€ word_selector.py
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ llm
â”‚   â”‚   â”œâ”€â”€ custom_gemini.py
â”‚   â”‚   â”œâ”€â”€ custom_mistral.py
â”‚   â”‚   â””â”€â”€ llm_router.py
â”‚   â”œâ”€â”€ llm_utils
â”‚   â”‚   â”œâ”€â”€ label_generator.py
â”‚   â”‚   â”œâ”€â”€ summarizer.py
â”‚   â”‚   â””â”€â”€ token_utils.py
â”‚   â””â”€â”€ models
â”‚       â”œâ”€â”€ CFDTM
â”‚       â”‚   â”œâ”€â”€ CFDTM.py
â”‚       â”‚   â”œâ”€â”€ Encoder.py
â”‚       â”‚   â”œâ”€â”€ ETC.py
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â””â”€â”€ UWE.py
â”‚       â”œâ”€â”€ DETM.py
â”‚       â”œâ”€â”€ DTM_trainer.py
â”‚       â””â”€â”€ dynamic_trainer.py
â”œâ”€â”€ data
â”‚   â””â”€â”€ Sample_data
â”‚       â”œâ”€â”€ CFDTM
â”‚       â”‚   â””â”€â”€ beta.npy
â”‚       â”œâ”€â”€ docs.jsonl
â”‚       â””â”€â”€ processed
â”‚           â”œâ”€â”€ length_stats.json
â”‚           â”œâ”€â”€ time2id.txt
â”‚           â”œâ”€â”€ train_bow.npz
â”‚           â”œâ”€â”€ train_texts.txt
â”‚           â”œâ”€â”€ train_times.txt
â”‚           â”œâ”€â”€ vocab.txt
â”‚           â””â”€â”€ word_embeddings.npz
â”œâ”€â”€ environment.yml
â”œâ”€â”€ LICENSE
â”œâ”€â”€ main.py
â””â”€â”€ requirements.txt
```

-----

## ğŸ“š Supporting Resources

We list below the datasets, codebases, and evaluation resources referenced or integrated into DTECT:

#### Datasets

- [ACL Anthology](https://aclanthology.org/)
- [UN General Debates](https://www.kaggle.com/datasets/unitednations/un-general-debates)
- [TCPD-IPD Finance](https://tcpd.ashoka.edu.in/question-hour/)

#### Dynamic Topic Modeling Codebases

- [DTM Trainer](https://github.com/bobxwu/TopMost/blob/main/topmost/trainers/dynamic/DTM_trainer.py)
- [DETM Model](https://github.com/bobxwu/TopMost/blob/main/topmost/models/dynamic/DETM.py)
- [CFDTM Model](https://github.com/bobxwu/TopMost/tree/main/topmost/models/dynamic/CFDTM)

#### Preprocessing Toolkit

- [OCTIS](https://github.com/MIND-Lab/OCTIS)

#### Evaluation Metrics

  * **Evaluating Dynamic Topic Models:** [https://github.com/CharuJames/Evaluating-Dynamic-Topic-Models](https://github.com/CharuJames/Evaluating-Dynamic-Topic-Models)

---

## ğŸ™ Acknowledgements

We would like to acknowledge the following open-source projects that were instrumental in the development of DTECT:

1.  **TopMost Toolkit:** [https://github.com/bobxwu/TopMost](https://github.com/bobxwu/TopMost)
2.  **OCTIS:** [https://github.com/MIND-Lab/OCTIS](https://github.com/MIND-Lab/OCTIS)

---
